# iTransformer架构详解

## 📐 数据流和维度变化

### 完整的数据流

```
输入数据: (batch_size, 1000, 40)
    ↓
输入ResNet: 对每个时间步的特征进行处理
    (batch_size, 1000, 40) -> reshape -> (batch_size*1000, 40)
    -> ResNet处理 -> (batch_size*1000, 512)
    -> reshape回 -> (batch_size, 1000, 512)
    ↓
输入Embedding: (batch_size, 1000, 512)
    ↓
Decoder层: 处理特征之间的关系（保持时间步维度）
    (batch_size, 1000, 512) -> ... -> (batch_size, 1000, 512)
    ↓
取最后一个时间步: (batch_size, 1000, 512) -> (batch_size, 512)
    ↓
输出ResNet: (batch_size, 512) -> ... -> (batch_size, 40)
    ↓
最终输出层: (batch_size, 40) -> (batch_size, 1)
```

---

## 🔍 关键问题解答

### Q1: ResNet的原始值（40）是加到第几层的？

**答案**：ResNet的第一个模块的输入维度 = 实际特征数量（会自动调整）

**工作原理**：
1. 训练脚本会自动检测实际特征数量（例如：40个特征）
2. ResNet的第一个模块会接收这个特征数量作为输入
3. 配置文件中第一个模块的`hidden_dims`会被自动调整

**示例**：
- 如果实际有40个特征，第一个模块：`input_dim=40, hidden_dims=[40, 40]`
- 如果实际有45个特征，第一个模块：`input_dim=45, hidden_dims=[45, 45]`（自动调整）

**注意**：配置文件中第一个模块的`hidden_dims`只是初始值，训练时会自动更新。

---

### Q2: 1000个时间步如何变成1个输出？

**答案**：通过**取最后一个时间步**实现

**代码位置**：`itransformer_decoder.py` 第365行
```python
x = x[:, -1, :]  # 取最后一个时间步
```

**详细说明**：

1. **输入**：`(batch_size, 1000, 40)`
   - 1000个时间步（1000天的历史数据）
   - 每个时间步40个特征

2. **经过Decoder后**：`(batch_size, 1000, 512)`
   - 仍然保持1000个时间步
   - 每个时间步512维特征

3. **取最后一个时间步**：`(batch_size, 512)`
   - `x[:, -1, :]` 表示取所有batch的第1000个（最后一个）时间步
   - 从1000个时间步变成1个向量

4. **为什么取最后一个时间步？**
   - 最后一个时间步包含了前面所有时间步的信息（通过注意力机制）
   - 这是时间序列预测的标准做法：用历史信息预测未来

---

### Q3: 输入1000天，预测第1030天，这个逻辑在哪里？

**答案**：在**数据生成阶段**已经处理好了

**数据流程**：

1. **索引文件生成**（`roll_generate_index_v0.2_20251212.py`）：
   - `input_row_start` 到 `input_row_end`：第1-1000天的数据
   - `target_row`：第1030天的数据（未来第30天）
   - 这个逻辑已经在索引生成时完成

2. **数据集加载**（`stock_dataset.py`）：
   - 根据索引文件读取第1-1000天的数据作为输入
   - 读取第1030天的收盘价作为目标

3. **模型训练**：
   - 输入：1000天的历史数据
   - 输出：预测第1030天的收盘价

**所以**：模型不需要知道"预测未来30天"，它只需要学习：
- 输入：1000天的特征数据
- 输出：某个特定日期的收盘价

---

## 🏗️ ResNet模块的维度处理

### 输入ResNet处理流程

**输入形状**：`(batch_size, 1000, 40)`

1. **Reshape**：`(batch_size, 1000, 40) -> (batch_size*1000, 40)`
   - 将1000个时间步"展平"，每个时间步独立处理

2. **ResNet处理**：`(batch_size*1000, 40) -> (batch_size*1000, 512)`
   - 对每个时间步的40个特征进行ResNet处理
   - 输出512维特征

3. **Reshape回**：`(batch_size*1000, 512) -> (batch_size, 1000, 512)`
   - 恢复时间步维度

**关键点**：
- ResNet**不处理时间步之间的关系**，只处理每个时间步的特征
- 时间步之间的关系由**Decoder**处理

### 输出ResNet处理流程

**输入形状**：`(batch_size, 512)`（已经取最后一个时间步）

1. **ResNet处理**：`(batch_size, 512) -> (batch_size, 40)`
   - 将512维特征逐步降维到40维

2. **最终输出层**：`(batch_size, 40) -> (batch_size, 1)`
   - 从40维降到1维（预测收盘价）

---

## 📊 维度变化总结

| 阶段 | 输入形状 | 输出形状 | 说明 |
|------|---------|---------|------|
| 原始输入 | (batch, 1000, 40) | - | 1000天，每天40个特征 |
| 输入ResNet | (batch, 1000, 40) | (batch, 1000, 512) | 每个时间步40->512 |
| Decoder | (batch, 1000, 512) | (batch, 1000, 512) | 处理特征关系 |
| 取最后时间步 | (batch, 1000, 512) | (batch, 512) | **1000->1的关键步骤** |
| 输出ResNet | (batch, 512) | (batch, 40) | 512->40 |
| 最终输出 | (batch, 40) | (batch, 1) | 40->1 |

---

## ✅ 自动调整功能

### 特征数量自动调整

**已实现**：
1. ✅ 自动检测实际特征数量
2. ✅ 自动更新模型配置中的`input_features`
3. ✅ 自动调整输入ResNet第一个模块的维度

**你不需要手动修改**：
- ❌ 不需要修改`itransformer_config.yaml`中的`input_features`
- ❌ 不需要修改输入ResNet第一个模块的`hidden_dims`

**程序会自动处理**：
- ✅ 检测到实际特征数后，自动更新配置
- ✅ 自动调整ResNet第一个模块的维度

---

## 🎯 配置建议

### 如果实际特征数是40

配置文件中的第一个模块：
```yaml
- name: "input_block_1"
  layers: 2
  hidden_dims: [40, 40]  # 这个会被自动调整，写40或45都可以
```

### 如果实际特征数是45

程序会自动：
1. 检测到45个特征
2. 更新第一个模块为：`hidden_dims: [45, 45]`
3. 继续后续的ResNet处理

---

## 📝 总结

1. **1000个时间步如何变成1？**
   - 通过取最后一个时间步：`x[:, -1, :]`
   - 最后一个时间步包含了所有历史信息

2. **预测第1030天的逻辑在哪里？**
   - 在数据生成阶段（索引文件）
   - 模型只负责学习输入到输出的映射

3. **ResNet的维度会自动调整吗？**
   - ✅ 会！第一个模块的输入维度会自动调整
   - ✅ 你只需要确保后续模块的维度匹配即可

4. **如何设置ResNet？**
   - 第一个模块的`hidden_dims`可以写任意值（会被自动调整）
   - 后续模块的维度需要匹配前一个模块的输出

