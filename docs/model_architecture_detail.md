# iTransformerDecoder 模型架构详解

## 3. 模型架构（iTransformerDecoder）

### 3.1 整体架构流程图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          输入数据 (原始特征)                                  │
│                    Shape: (batch_size, seq_len, num_features)                │
│                        例如: (32, 100, 40)                                   │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段1】输入ResNet模块（可选）                          │
│                                                                              │
│  对每个时间步的特征进行残差全连接处理，逐步扩展特征维度                       │
│                                                                              │
│  Reshape: (batch, seq_len, num_features) → (batch*seq_len, num_features)    │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块1: ResidualFCBlock                                       │           │
│  │   40 → [Linear(40→40) → GELU → Dropout] × 2 → 40            │           │
│  │   残差连接: 40 → 40                                          │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块2: ResidualFCBlock                                       │           │
│  │   40 → [Linear(40→128) → GELU → Dropout] → 128              │           │
│  │   残差投影: 40 → 128                                         │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块3: ResidualFCBlock                                       │           │
│  │   128 → [Linear(128→128) → GELU → Dropout] × 2 → 128        │           │
│  │   残差连接: 128 → 128                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块4: ResidualFCBlock                                       │           │
│  │   128 → [Linear(128→256) → GELU → Dropout] → 256            │           │
│  │   残差投影: 128 → 256                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块5: ResidualFCBlock                                       │           │
│  │   256 → [Linear(256→256) → GELU → Dropout] × 2 → 256        │           │
│  │   残差连接: 256 → 256                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块6: ResidualFCBlock                                       │           │
│  │   256 → [Linear(256→512) → GELU → Dropout] → 512            │           │
│  │   残差投影: 256 → 512                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块7: ResidualFCBlock                                       │           │
│  │   512 → [Linear(512→512) → GELU → Dropout] × 2 → 512         │           │
│  │   残差连接: 512 → 512                                         │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│  Reshape: (batch*seq_len, 512) → (batch, seq_len, 512)                     │
│                                                                              │
│  输出 Shape: (batch_size, seq_len, 512)                                     │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段2】输入Embedding层                                 │
│                                                                              │
│  Linear(512 → d_model)                                                       │
│  将特征维度映射到模型隐藏维度                                                 │
│                                                                              │
│  输入: (batch, seq_len, 512)                                                │
│  输出: (batch, seq_len, d_model)                                             │
│        例如: (32, 100, 512)                                                 │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段3】转置操作（iTransformer核心）                    │
│                                                                              │
│  这是iTransformer的关键创新：将注意力从时间维度转向特征维度                  │
│                                                                              │
│  transpose(1, 2): (batch, seq_len, d_model) → (batch, d_model, seq_len)    │
│                                                                              │
│  含义：                                                                      │
│  - 原来: 每个时间步是一个token，注意力在时间步之间                           │
│  - 现在: 每个特征维度是一个token，注意力在特征之间                           │
│                                                                              │
│  输出: (batch_size, d_model, seq_len)                                       │
│        例如: (32, 512, 100)                                                │
│                                                                              │
│  注意：此时d_model=512个"特征token"，每个token有seq_len=100个时间步的值      │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段4】Decoder层（多层堆叠）                            │
│                                                                              │
│  重复执行6次（n_layers=6）                                                   │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────┐          │
│  │                    Decoder Layer (×6)                          │          │
│  │                                                                │          │
│  │  ┌────────────────────────────────────────────────────┐       │          │
│  │  │  子层1: 自注意力机制 (Self-Attention)              │       │          │
│  │  │                                                    │       │          │
│  │  │  输入: (batch, d_model, seq_len)                  │       │          │
│  │  │  例如: (32, 512, 100)                             │       │          │
│  │  │                                                    │       │          │
│  │  │  1. LayerNorm (Pre-Norm)                          │       │          │
│  │  │     Shape: (batch, d_model, seq_len)              │       │          │
│  │  │                                                    │       │          │
│  │  │     LayerNorm公式：                                │       │          │
│  │  │     y = (x - E[x]) / √(Var[x] + ε) * γ + β       │       │          │
│  │  │                                                    │       │          │
│  │  │     其中：                                         │       │          │
│  │  │     - E[x]: 在最后一个维度(seq_len)上计算均值      │       │          │
│  │  │     - Var[x]: 在最后一个维度上计算方差            │       │          │
│  │  │     - ε: 防止除零的小常数(默认1e-5)               │       │          │
│  │  │     - γ: 可学习的缩放参数(初始化为1)               │       │          │
│  │  │     - β: 可学习的偏移参数(初始化为0)              │       │          │
│  │  │                                                    │       │          │
│  │  │     对于输入 (batch, d_model, seq_len):            │       │          │
│  │  │     - 对每个(batch, d_model)位置，在seq_len维度归一化│       │          │
│  │  │     - 即：对每个特征token的100个时间步值进行归一化  │       │          │
│  │  │                                                    │       │          │
│  │  │     为什么需要LayerNorm？                          │       │          │
│  │  │     1. 虽然输入数据已标准化，但LayerNorm作用不同：  │       │          │
│  │  │        - 输入标准化：在特征层面(不同特征的均值和方差)│       │          │
│  │  │        - LayerNorm：在每个样本的最后一个维度上归一化│       │          │
│  │  │     2. LayerNorm有可学习参数γ和β，可以自适应调整   │       │          │
│  │  │     3. 稳定训练：防止激活值过大/过小，帮助梯度流动   │       │          │
│  │  │     4. 加速收敛：归一化后的数据分布更稳定           │       │          │
│  │  │                                                    │       │          │
│  │  │     能否去掉LayerNorm？                            │       │          │
│  │  │     - 理论上可以，但不推荐                         │       │          │
│  │  │     - 去掉后训练可能不稳定，需要更小的学习率        │       │          │
│  │  │     - LayerNorm是Transformer架构的关键组件          │       │          │
│  │  │                                                    │       │          │
│  │  │  2. MultiheadAttention                             │       │          │
│  │  │     - embed_dim = seq_len (100)                   │       │          │
│  │  │     - num_heads = 4                                │       │          │
│  │  │     - 注意力在特征维度上进行                        │       │          │
│  │  │     - 每个特征token关注其他特征token                │       │          │
│  │  │     - 无因果掩码（use_causal_mask=False）           │       │          │
│  │  │                                                    │       │          │
│  │  │  注意力计算过程：                                    │       │          │
│  │  │    Q, K, V = Linear(x)                            │       │          │
│  │  │    Attention(Q, K, V) = softmax(QK^T / √d_k) V    │       │          │
│  │  │    其中注意力矩阵形状: (batch, n_heads, seq_len, seq_len)│          │
│  │  │                                                    │       │          │
│  │  │  3. Dropout (0.1)                                  │       │          │
│  │  │                                                    │       │          │
│  │  │  4. 残差连接                                        │       │          │
│  │  │     output = input + dropout(attention_output)     │       │          │
│  │  │                                                    │       │          │
│  │  │  输出: (batch, d_model, seq_len)                  │       │          │
│  │  └────────────────────────────────────────────────────┘       │          │
│  │                          │                                     │          │
│  │                          ▼                                     │          │
│  │  ┌────────────────────────────────────────────────────┐       │          │
│  │  │  子层2: 前馈网络 (Feed-Forward Network)             │       │          │
│  │  │                                                    │       │          │
│  │  │  输入: (batch, d_model, seq_len)                  │       │          │
│  │  │                                                    │       │          │
│  │  │  1. LayerNorm (Pre-Norm)                          │       │          │
│  │  │     同上，在最后一个维度(seq_len)上归一化          │       │          │
│  │  │                                                    │       │          │
│  │  │  2. 前馈网络                                        │       │          │
│  │  │     Linear(d_model → d_ff)                        │       │          │
│  │  │     512 → 2048                                     │       │          │
│  │  │                                                    │       │          │
│  │  │  3. 激活函数                                        │       │          │
│  │  │     GELU(x)                                       │       │          │
│  │  │                                                    │       │          │
│  │  │  4. Dropout (0.1)                                  │       │          │
│  │  │                                                    │       │          │
│  │  │  5. Linear(d_ff → d_model)                        │       │          │
│  │  │     2048 → 512                                     │       │          │
│  │  │                                                    │       │          │
│  │  │  6. 残差连接                                        │       │          │
│  │  │     output = input + ff_output                    │       │          │
│  │  │                                                    │       │          │
│  │  │  输出: (batch, d_model, seq_len)                  │       │          │
│  │  └────────────────────────────────────────────────────┘       │          │
│  │                                                                │          │
│  │  每层输出: (batch, d_model, seq_len)                          │          │
│  │  例如: (32, 512, 100)                                         │          │
│  └──────────────────────────────────────────────────────────────┘          │
│                                                                              │
│  最终输出: (batch_size, d_model, seq_len)                                   │
│           例如: (32, 512, 100)                                              │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段5】转置回来                                        │
│                                                                              │
│  transpose(1, 2): (batch, d_model, seq_len) → (batch, seq_len, d_model)    │
│                                                                              │
│  输出: (batch_size, seq_len, d_model)                                       │
│        例如: (32, 100, 512)                                                │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段6】时间步聚合层（替代直接取最后时间步）              │
│                                                                              │
│  使用两个全连接层学习如何聚合所有时间步的信息，而不是简单取最后一个           │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │  全连接层1: 时间步聚合                                        │           │
│  │    Linear(seq_len → 1)                                       │           │
│  │    将100个时间步压缩为1个                                     │           │
│  │                                                              │           │
│  │    输入: (batch, seq_len, d_model)                          │           │
│  │    例如: (32, 100, 512)                                     │           │
│  │                                                              │           │
│  │    操作:                                                     │           │
│  │    - 对每个特征维度，学习如何加权所有时间步                  │           │
│  │    - 可以学习到"哪些时间步更重要"                            │           │
│  │                                                              │           │
│  │    输出: (batch, 1, d_model)                                │           │
│  │    例如: (32, 1, 512)                                       │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │  全连接层2: 特征维度压缩                                      │           │
│  │    Linear(d_model → 1)                                       │           │
│  │    将512个特征维度压缩为1个                                   │           │
│  │                                                              │           │
│  │    输入: (batch, 1, d_model)                                │           │
│  │    例如: (32, 1, 512)                                       │           │
│  │                                                              │           │
│  │    操作:                                                     │           │
│  │    - 学习如何组合所有特征维度                                │           │
│  │    - 可以学习到"哪些特征更重要"                              │           │
│  │                                                              │           │
│  │    输出: (batch, 1, 1)                                      │           │
│  │    例如: (32, 1, 1)                                         │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │  Squeeze操作                                                   │           │
│  │    squeeze(1) 或 squeeze(-1)                                 │           │
│  │    移除维度1                                                   │           │
│  │                                                              │           │
│  │    输出: (batch, 1) 或 (batch,)                              │           │
│  │    例如: (32, 1)                                             │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                                                              │
│  优势：                                                                      │
│  - 学习型聚合：模型可以学习如何加权所有时间步，而不是硬编码取最后一个        │
│  - 更灵活：可以学习到"中间某个时间步更重要"的模式                             │
│  - 信息保留：利用所有时间步的信息，而不是只用一个                            │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段7】输出ResNet模块（可选）                          │
│                                                                              │
│  对最终特征进行残差全连接处理，逐步降低特征维度                               │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块1: ResidualFCBlock                                       │           │
│  │   512 → [Linear(512→512) → GELU → Dropout] × 2 → 512         │           │
│  │   残差连接: 512 → 512                                         │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块2: ResidualFCBlock                                       │           │
│  │   512 → [Linear(512→256) → GELU → Dropout] → 256            │           │
│  │   残差投影: 512 → 256                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块3: ResidualFCBlock                                       │           │
│  │   256 → [Linear(256→256) → GELU → Dropout] × 2 → 256        │           │
│  │   残差连接: 256 → 256                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块4: ResidualFCBlock                                       │           │
│  │   256 → [Linear(256→128) → GELU → Dropout] → 128            │           │
│  │   残差投影: 256 → 128                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块5: ResidualFCBlock                                       │           │
│  │   128 → [Linear(128→128) → GELU → Dropout] × 2 → 128        │           │
│  │   残差连接: 128 → 128                                        │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块6: ResidualFCBlock                                       │           │
│  │   128 → [Linear(128→40) → GELU → Dropout] → 40              │           │
│  │   残差投影: 128 → 40                                         │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                 │                                            │
│                                 ▼                                            │
│  ┌──────────────────────────────────────────────────────────────┐           │
│  │ 模块7: ResidualFCBlock                                       │           │
│  │   40 → [Linear(40→40) → GELU → Dropout] × 2 → 40            │           │
│  │   残差连接: 40 → 40                                          │           │
│  └──────────────────────────────────────────────────────────────┘           │
│                                                                              │
│  输出 Shape: (batch_size, 40)                                               │
│        例如: (32, 40)                                                       │
└────────────────────────────────┬────────────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                      【阶段8】最终输出层                                     │
│                                                                              │
│  Sequential(                                                               │
│    Linear(40 → 40),        # 隐藏层                                        │
│    GELU(),                  # 激活函数                                      │
│    Dropout(0.1),            # Dropout                                       │
│    Linear(40 → 1)           # 输出层                                        │
│  )                                                                          │
│                                                                              │
│  输入: (batch, 40)                                                          │
│  输出: (batch, 1)                                                           │
│        例如: (32, 1)                                                        │
│                                                                              │
│  这就是最终的预测值！                                                        │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 关键组件详解

#### 3.2.1 ResidualFCBlock（残差全连接块）

```
输入 x: (..., input_dim)
     │
     ├─────────────────┐
     │                 │
     ▼                 │
┌─────────────┐        │
│   Linear    │        │
│ (input_dim  │        │
│  → hidden)  │        │
└──────┬──────┘        │
       │               │
       ▼               │
┌─────────────┐        │
│   GELU      │        │
└──────┬──────┘        │
       │               │
       ▼               │
┌─────────────┐        │
│  Dropout    │        │
└──────┬──────┘        │
       │               │
       │ (多层重复)     │
       │               │
       ▼               │
┌─────────────┐        │
│  输出       │        │
└──────┬──────┘        │
       │               │
       │               │
       ▼               ▼
    ┌─────┐      ┌──────────┐
    │  +  │ ←────│ Residual │
    └──┬──┘      │ Projection│
       │         └──────────┘
       │
       ▼
   输出: (..., output_dim)
```

**特点：**
- 如果 input_dim ≠ output_dim，使用残差投影层（Residual Projection）
- 如果 input_dim = output_dim，直接残差连接
- 每层都有激活函数和Dropout，防止过拟合

#### 3.2.2 DecoderLayer（解码器层）

```
输入: (batch, d_model, seq_len)
     │
     ├─────────────────────────────────────┐
     │                                     │
     ▼                                     │
┌─────────────────┐                        │
│  LayerNorm      │                        │
│  (Pre-Norm)     │                        │
└──────┬──────────┘                        │
       │                                     │
       ▼                                     │
┌─────────────────┐                        │
│ Self-Attention  │                        │
│ (特征维度)      │                        │
│                 │                        │
│ Q, K, V 计算    │                        │
│ Attention权重   │                        │
└──────┬──────────┘                        │
       │                                     │
       ▼                                     │
┌─────────────────┐                        │
│   Dropout       │                        │
└──────┬──────────┘                        │
       │                                     │
       │                                     │
       ▼                                     │
    ┌─────┐                                 │
    │  +  │ ←──────────────────────────────┘
    └──┬──┘
       │
       ▼
┌─────────────────┐
│  LayerNorm      │
│  (Pre-Norm)     │
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  FFN            │
│  Linear(512→2048)│
│  GELU           │
│  Dropout        │
│  Linear(2048→512)│
└──────┬──────────┘
       │
       │
       ▼
    ┌─────┐
    │  +  │ ←─────── 残差连接
    └──┬──┘
       │
       ▼
输出: (batch, d_model, seq_len)
```

**关键点：**
- **Pre-Norm架构**：先归一化，再计算注意力/FFN
- **自注意力在特征维度**：每个特征token关注其他特征token
- **多头注意力**：4个头，每个头关注不同的特征关系
- **残差连接**：帮助梯度流动，训练更稳定

#### 3.2.3 iTransformer的核心创新

**传统Transformer（时间序列预测）：**
```
输入: (batch, seq_len, features)
      │
      ▼
注意力在时间维度: 时间步1 ←→ 时间步2 ←→ 时间步3 ←→ ...
```

**iTransformer：**
```
输入: (batch, seq_len, features)
      │
      ▼
转置: (batch, features, seq_len)
      │
      ▼
注意力在特征维度: 特征1 ←→ 特征2 ←→ 特征3 ←→ ...
```

**为什么这样做？**
- 时间序列中，**特征之间的关系**往往比时间步之间的关系更重要
- 例如：股价、成交量、MACD等指标之间的相关性
- 通过特征维度的注意力，模型可以学习到"哪些特征组合能更好地预测目标"

### 3.3 数据形状变化总览

| 阶段 | 操作 | 输入形状 | 输出形状 | 说明 |
|------|------|----------|----------|------|
| 输入 | 原始数据 | (32, 100, 40) | (32, 100, 40) | batch=32, seq_len=100, features=40 |
| 输入ResNet | Reshape + 7个模块 | (32, 100, 40) | (32, 100, 512) | 特征维度扩展：40→128→256→512 |
| 输入Embedding | Linear | (32, 100, 512) | (32, 100, 512) | 映射到d_model维度 |
| **转置** | transpose(1,2) | (32, 100, 512) | **(32, 512, 100)** | **iTransformer核心** |
| Decoder层×6 | 自注意力+FFN | (32, 512, 100) | (32, 512, 100) | 特征维度上的注意力 |
| **转置回来** | transpose(1,2) | (32, 512, 100) | (32, 100, 512) | 恢复时间维度 |
| 取最后时间步 | x[:, -1, :] | (32, 100, 512) | (32, 512) | 只保留最后一个时间步 |
| 输出ResNet | 7个模块 | (32, 512) | (32, 40) | 特征维度压缩：512→256→128→40 |
| 最终输出层 | Linear×2 | (32, 40) | **(32, 1)** | 预测值 |

### 3.4 参数统计（示例配置）

- **输入ResNet模块**：7个ResidualFCBlock，约 1.2M 参数
- **输入Embedding**：512×512 = 262K 参数
- **Decoder层（6层）**：
  - 每层自注意力：约 2.1M 参数
  - 每层FFN：约 2.6M 参数
  - 6层总计：约 28M 参数
- **输出ResNet模块**：7个ResidualFCBlock，约 1.2M 参数
- **最终输出层**：约 1.6K 参数
- **总参数**：约 31M 参数

### 3.5 训练时的数据流

```
DataLoader
    │
    ▼
(batch_size, seq_len, num_features)  ← 从数据集加载
    │
    ▼
[输入ResNet] → (batch, seq_len, 512)
    │
    ▼
[输入Embedding] → (batch, seq_len, 512)
    │
    ▼
[转置] → (batch, 512, seq_len)
    │
    ▼
[Decoder×6] → (batch, 512, seq_len)
    │
    ▼
[转置回来] → (batch, seq_len, 512)
    │
    ▼
[取最后时间步] → (batch, 512)
    │
    ▼
[输出ResNet] → (batch, 40)
    │
    ▼
[最终输出层] → (batch, 1)  ← 预测值
    │
    ▼
与真实值 (batch, 1) 计算损失
    │
    ▼
反向传播更新参数
```

### 3.6 关键设计理念

1. **特征维度注意力**：iTransformer的核心，让模型关注特征间关系而非时间关系
2. **ResNet风格残差**：输入和输出都使用残差连接，帮助梯度流动和特征学习
3. **渐进式特征变换**：输入ResNet逐步扩展，输出ResNet逐步压缩，平滑特征空间
4. **Pre-Norm架构**：归一化在计算之前，训练更稳定
5. **多层堆叠**：6层Decoder，逐步提取高级特征表示

