#!/Users/juntongchen/Downloads/è´¢åŠ¡æ•°æ®AIè®­ç»ƒ_æµ‹è¯•ä»£ç _20251017/mac-test-venv/bin/python
# -*- coding: utf-8 -*-
"""
ç»¼åˆè´¢åŠ¡ + Kçº¿æ•°æ®æŠ“å–è„šæœ¬ v0.3

åŠŸèƒ½ï¼š
1. è¯»å–è‚¡ç¥¨åˆ—è¡¨ï¼Œé€å®¶å…¬å¸æ‹‰å–è´¢åŠ¡æ•°æ®ä¸æ—¥K/å‘¨Kæ•°æ®
2. æ¯å®¶å…¬å¸ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹Excelï¼ŒåŒ…å«è´¢åŠ¡æ•°æ®ã€æ—¥Kã€å‘¨Kã€è´¢æŠ¥Ã—æ—¥Kå¯¹é½ç»“æœ
3. è‡ªåŠ¨ç”Ÿæˆcompany_indexç´¢å¼•æ–‡ä»¶ä¸progressæ–­ç‚¹æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
4. è´¢æŠ¥-æ—¥Kå¯¹é½ï¼šä»¥"è´¢æŠ¥å‘å¸ƒæ—¥æœŸï¼šè´¢æŠ¥å…¬å¼€æ—¥æœŸã€å…¬å‘Šæ—¥æœŸ"ä¸ºåŸºå‡†ï¼Œå‘å‰å¡«å……è¦†ç›–æ‰€æœ‰æ—¥Kæ—¥æœŸ

v0.3 æ›´æ–°ï¼š
- åˆ‡æ¢åˆ° eastmoney_v0.7 æ¨¡å—ï¼ˆå¢å¼ºè¯·æ±‚å¤´ + è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼‰
- è¯¦ç»†è®°å½•æ‰€æœ‰Kçº¿è·å–å¤±è´¥åŸå› ï¼Œä¿å­˜åˆ°ç‹¬ç«‹æ—¥å¿—æ–‡ä»¶
- åœ¨å…¬å¸ç´¢å¼•ä¸­å¢åŠ "æ—¥Kå¤±è´¥åŸå› "å’Œ"å‘¨Kå¤±è´¥åŸå› "å­—æ®µ
- è¿ç»­5æ¬¡Kçº¿è·å–å¤±è´¥åˆ™åœæ­¢ï¼ˆå·²åœ¨v0.2å®ç°ï¼Œv0.3ä¿æŒï¼‰
- æˆåŠŸåå¤±è´¥è®¡æ•°å½’é›¶ï¼ˆå·²åœ¨v0.2å®ç°ï¼Œv0.3ä¿æŒï¼‰
- æ‰¹é‡æš‚åœæ§åˆ¶ï¼šæ¯æˆåŠŸå¤„ç†nå®¶å…¬å¸åï¼Œæš‚åœmåˆ†é’Ÿï¼ˆå¯é…ç½®ï¼‰
- è´¢åŠ¡æ•°æ®å¤±è´¥è·³è¿‡ç­–ç•¥ï¼šå½“è´¢åŠ¡æ•°æ®è·å–å¤±è´¥æ—¶ï¼Œç›´æ¥è·³è¿‡è¯¥å…¬å¸çš„Kçº¿æ•°æ®è·å–
"""

import os
import sys
import json
import time
import random
import traceback
import importlib.util
from datetime import datetime, timedelta

import pandas as pd


# =========================== é…ç½®åŒºåŸŸï¼ˆå¯ä¿®æ”¹ï¼‰ ===========================
# è‚¡ç¥¨åˆ—è¡¨åŠç­›é€‰
STOCK_LIST_FILE = "è‚¡ç¥¨ä»£ç æ±‡æ€»-é™ˆä¿ŠåŒ-20251118.xlsx"
SHEET_NAME = 0                    # Excel sheetç´¢å¼•æˆ–åç§°
FILTER_BY_COLUMN = "å¤„ç†"          # ä¸ºNoneåˆ™ä¸ç­›é€‰
FILTER_CODES = None               # å¦‚ ['600519', '00700.HK']ï¼Œä¸ºNoneå¤„ç†å…¨éƒ¨

# è´¢åŠ¡æ˜ å°„æ–‡ä»¶
MAPPING_FILE_PATH = "ä¸œæ–¹è´¢å¯Œè´¢åŠ¡æ•°æ®APIæ˜ å°„æœ€ç»ˆç‰ˆ-é™ˆä¿ŠåŒ-20251030.xlsx"

# Kçº¿æŠ“å–æ§åˆ¶
FETCH_DAILY = True
FETCH_WEEKLY = True

# Kçº¿æ—¶é—´èŒƒå›´é»˜è®¤å€¼ï¼ˆæ ¼å¼ï¼š"YYYYMMDD-YYYYMMDD" æˆ– Noneè¡¨ç¤ºä½¿ç”¨365å¤©ï¼‰
# æ³¨ï¼šå¦‚æœAPIè¿”å›çš„æ•°æ®ä¸è¶³è®¾ç½®çš„èŒƒå›´ï¼Œåˆ™è¿”å›å®é™…å¯ç”¨çš„æ•°æ®é‡
DEFAULT_DAILY_RANGE = "20200101-20251115"    # æ—¥Ké»˜è®¤èŒƒå›´
DEFAULT_WEEKLY_RANGE = "20200101-20251115"   # å‘¨Ké»˜è®¤èŒƒå›´

DELAY_SECONDS = (15, 20)          # æ¯æ¬¡è¯·æ±‚éšæœºå»¶è¿ŸåŒºé—´
MAX_CONSECUTIVE_FAILURES = 5      # è¿ç»­å¤±è´¥åˆ¤å®šIPå°ç¦

# æ‰¹é‡æš‚åœæ§åˆ¶ï¼ˆæ¯æˆåŠŸnå®¶åæš‚åœmåˆ†é’Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹è¯·æ±‚ï¼‰
PAUSE_AFTER_N_SUCCESS = 20        # æ¯æˆåŠŸå¤„ç†nå®¶å…¬å¸åæš‚åœï¼ˆè®¾ä¸º0åˆ™ä¸å¯ç”¨ï¼‰
PAUSE_MINUTES = 20                 # æš‚åœæ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰

# è¾“å‡ºä¸æ–­ç‚¹
OUTPUT_BASE_DIR = "."
RESUME_PROGRESS_FILE = '/Users/juntongchen/Downloads/è´¢åŠ¡æ•°æ®AIè®­ç»ƒ_æµ‹è¯•ä»£ç _20251017/download_financial&candle_v0.3_20251119_155305/progress_20251119_155305.json'       # è‹¥éœ€ç»­ä¼ ï¼Œå¡«å…¥progressæ–‡ä»¶ç»å¯¹æˆ–ç›¸å¯¹è·¯å¾„

# å¯¹é½è®¾ç½®
NOTICE_COL = "è´¢æŠ¥å‘å¸ƒæ—¥æœŸï¼šè´¢æŠ¥å…¬å¼€æ—¥æœŸã€å…¬å‘Šæ—¥æœŸ"

# ========================================================================


# ---------- åŠ¨æ€å¯¼å…¥ä¾èµ–è„šæœ¬ ----------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# financial_data_mapper_v3.21_batch_period.py
_financial_module_path = os.path.join(BASE_DIR, "financial_data_mapper_v3.21_batch_period.py")
_financial_spec = importlib.util.spec_from_file_location("financial_mapper_v321", _financial_module_path)
financial_module = importlib.util.module_from_spec(_financial_spec)
_financial_spec.loader.exec_module(financial_module)

# eastmoney_v0.7ï¼ˆç”¨äºæ—¥K/å‘¨Kï¼‰
sys.path.append(os.path.join(BASE_DIR, "src"))
sys.path.append(os.path.join(BASE_DIR, "src", "providers"))
_eastmoney_path = os.path.join(BASE_DIR, "src", "providers", "eastmoney_v0.7.py")
_eastmoney_spec = importlib.util.spec_from_file_location("eastmoney_v0_7", _eastmoney_path)
eastmoney_module = importlib.util.module_from_spec(_eastmoney_spec)
_eastmoney_spec.loader.exec_module(eastmoney_module)
get_historical_data = eastmoney_module.get_historical_data


class IPBlockedException(Exception):
    """IPè¢«å°ç¦å¼‚å¸¸"""


def ensure_output_dir(base_dir):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = os.path.join(base_dir, f"download_financial&candle_v0.3_{timestamp}")
    os.makedirs(out_dir, exist_ok=True)
    return out_dir, timestamp


def parse_date_range(date_str):
    """è§£æ 'YYYYMMDD-YYYYMMDD' å­—ç¬¦ä¸²"""
    if pd.isna(date_str) or not date_str:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)
        return start_date.strftime("%Y%m%d"), end_date.strftime("%Y%m%d"), 365

    try:
        date_str = str(date_str).strip()
        parts = date_str.split('-')
        if len(parts) != 2:
            return None
        start = datetime.strptime(parts[0].strip(), "%Y%m%d")
        end = datetime.strptime(parts[1].strip(), "%Y%m%d")
        days = (end - start).days
        if days <= 0:
            return None
        return parts[0].strip(), parts[1].strip(), days
    except Exception:
        return None


def get_random_delay():
    if isinstance(DELAY_SECONDS, (list, tuple)):
        return random.uniform(float(DELAY_SECONDS[0]), float(DELAY_SECONDS[1]))
    return float(DELAY_SECONDS)




class CandleFetcher:
    """Kçº¿æ•°æ®è·å–å™¨ï¼ˆv0.3 å¢å¼ºç‰ˆï¼‰
    
    v0.3 æ–°å¢ï¼š
    - è¯¦ç»†è®°å½•æ¯æ¬¡å¤±è´¥çš„åŸå› ã€æ—¶é—´ã€è‚¡ç¥¨ä»£ç ç­‰ä¿¡æ¯
    - æä¾›å¤±è´¥æ—¥å¿—åˆ—è¡¨ä¾›å¤–éƒ¨ä¿å­˜
    """
    def __init__(self):
        self.consecutive_failures = 0
        self.max_consecutive_failures = MAX_CONSECUTIVE_FAILURES
        self.ip_blocked = False
        # v0.3 æ–°å¢ï¼šå¤±è´¥æ—¥å¿—è®°å½•
        self.failure_log = []

    def _is_ip_blocked(self, error):
        message = str(error).lower()
        keywords = ["403", "429", "forbidden", "too many requests", "è®¿é—®é¢‘ç¹", "ipé™åˆ¶", "blocked", "rate limit"]
        if any(keyword in message for keyword in keywords):
            return True
        if isinstance(error, ConnectionRefusedError):
            return True
        return False

    def _log_failure(self, stock_code, market, period, error_msg, error_type="unknown"):
        """è®°å½•å¤±è´¥è¯¦æƒ…åˆ°æ—¥å¿—"""
        log_entry = {
            "æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "è‚¡ç¥¨ä»£ç ": stock_code,
            "å¸‚åœº": market,
            "å‘¨æœŸ": period,
            "é”™è¯¯ç±»å‹": error_type,
            "é”™è¯¯ä¿¡æ¯": error_msg,
            "è¿ç»­å¤±è´¥æ¬¡æ•°": self.consecutive_failures
        }
        self.failure_log.append(log_entry)

    def fetch(self, stock_code, market, period, date_range=None):
        """
        è·å–Kçº¿æ•°æ®ï¼Œå¤±è´¥æ—¶ç´¯åŠ è®¡æ•°å™¨å¹¶è®°å½•è¯¦ç»†åŸå› 
        
        v0.3 ä¿®æ”¹ï¼š
        - æ‰€æœ‰å¤±è´¥åœºæ™¯éƒ½ä¼šè®°å½•åˆ° failure_log
        - è¿”å›å€¼å¢åŠ å¤±è´¥åŸå› å­—ç¬¦ä¸²
        
        Returns:
            tuple: (DataFrame, count, error_message)
                - DataFrame: Kçº¿æ•°æ®ï¼ˆå¤±è´¥æ—¶ä¸ºNoneï¼‰
                - count: æ•°æ®è¡Œæ•°ï¼ˆå¤±è´¥æ—¶ä¸º0ï¼‰
                - error_message: å¤±è´¥åŸå› ï¼ˆæˆåŠŸæ—¶ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
        """
        if not (FETCH_DAILY or FETCH_WEEKLY):
            return None, 0, ""

        parsed = parse_date_range(date_range) if date_range else None
        if parsed is None:
            parsed = parse_date_range(None)
        _, _, days = parsed

        try:
            df = get_historical_data(stock_code=stock_code, market=market, period=period, days=days)
            if df is not None and not df.empty:
                # æˆåŠŸè·å–æ•°æ®ï¼Œé‡ç½®è®¡æ•°å™¨
                self.consecutive_failures = 0
                return df, len(df), ""
            
            # æ•°æ®ä¸ºç©ºä¹Ÿç®—å¤±è´¥
            self.consecutive_failures += 1
            error_msg = f"è·å–{period}æ•°æ®ä¸ºç©º"
            self._log_failure(stock_code, market, period, error_msg, "ç©ºæ•°æ®")
            print(f"  âœ— {error_msg}ï¼ˆè¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼‰")
            
            if self.consecutive_failures >= self.max_consecutive_failures:
                self.ip_blocked = True
                raise IPBlockedException(
                    f"Kçº¿æ•°æ®è¿ç»­{self.consecutive_failures}æ¬¡è·å–å¤±è´¥ï¼ˆåŒ…æ‹¬ç©ºæ•°æ®ï¼‰ï¼Œç–‘ä¼¼IPè¢«å°"
                )
            
            return None, 0, error_msg
            
        except IPBlockedException:
            # IPå°ç¦å¼‚å¸¸ç›´æ¥å‘ä¸ŠæŠ›å‡º
            raise
            
        except Exception as exc:
            # æ£€æµ‹IPå°ç¦ç‰¹å¾
            if self._is_ip_blocked(exc):
                self.ip_blocked = True
                error_msg = f"æ£€æµ‹åˆ°IPå°ç¦ç‰¹å¾ï¼š{exc}"
                self._log_failure(stock_code, market, period, error_msg, "IPå°ç¦")
                raise IPBlockedException(error_msg)

            # å…¶ä»–å¼‚å¸¸ä¹Ÿç´¯åŠ è®¡æ•°å™¨
            self.consecutive_failures += 1
            error_msg = str(exc)
            self._log_failure(stock_code, market, period, error_msg, "å¼‚å¸¸")
            print(f"  âœ— è·å–{period}æ•°æ®å¤±è´¥ï¼ˆè¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼‰ï¼š{exc}")
            
            if self.consecutive_failures >= self.max_consecutive_failures:
                self.ip_blocked = True
                raise IPBlockedException(
                    f"Kçº¿æ•°æ®è¿ç»­{self.consecutive_failures}æ¬¡è¯·æ±‚å¤±è´¥ï¼Œç–‘ä¼¼IPè¢«å°ï¼š{exc}"
                )
            
            return None, 0, error_msg

    def get_failure_log_df(self):
        """å°†å¤±è´¥æ—¥å¿—è½¬æ¢ä¸ºDataFrame"""
        if not self.failure_log:
            return pd.DataFrame()
        return pd.DataFrame(self.failure_log)


def align_financial_with_daily(fin_df, daily_df):
    if fin_df is None or fin_df.empty or daily_df is None or daily_df.empty:
        return pd.DataFrame()

    df_fin = fin_df.copy()
    if NOTICE_COL not in df_fin.columns:
        print("  âš  è´¢åŠ¡æ•°æ®ç¼ºå°‘å…¬å‘Šæ—¥æœŸåˆ—ï¼Œæ— æ³•å¯¹é½")
        return pd.DataFrame()

    df_fin[NOTICE_COL] = pd.to_datetime(df_fin[NOTICE_COL], errors="coerce")
    df_fin = df_fin.dropna(subset=[NOTICE_COL])
    if df_fin.empty:
        return pd.DataFrame()

    df_daily = daily_df.copy()
    df_daily["æ—¥æœŸ"] = pd.to_datetime(df_daily["æ—¥æœŸ"], errors="coerce")
    df_daily = df_daily.dropna(subset=["æ—¥æœŸ"])
    if df_daily.empty:
        return pd.DataFrame()

    # æŒ‰æ—¶é—´æ’åºï¼Œä¸å†æå‰è¿‡æ»¤è´¢æŠ¥èŒƒå›´
    df_fin = df_fin.sort_values(NOTICE_COL).reset_index(drop=True)
    df_daily = df_daily.sort_values("æ—¥æœŸ").reset_index(drop=True)

    # merge_asof è‡ªåŠ¨åŒ¹é…ï¼šæ¯æ¡æ—¥Kæ‰¾æœ€è¿‘çš„ä¸æ™šäºå®ƒçš„è´¢æŠ¥ï¼ˆåŒ…æ‹¬æ—©äºæ—¥Kèµ·ç‚¹çš„è´¢æŠ¥ï¼‰
    merged = pd.merge_asof(
        df_daily,
        df_fin,
        left_on="æ—¥æœŸ",
        right_on=NOTICE_COL,
        direction="backward"
    )

    # å¯¹äºæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•è´¢æŠ¥çš„æ—¥Kè¡Œï¼ˆæœ€æ—©è´¢æŠ¥ä¹‹å‰ä¸”æ— æ›´æ—©è´¢æŠ¥å¯ç”¨ï¼‰ï¼Œå¡«å……0
    # åˆ¤æ–­ä¾æ®ï¼šNOTICE_COLåˆ—ä¸ºç©º
    no_match_mask = merged[NOTICE_COL].isna()
    if no_match_mask.any():
        financial_cols = [col for col in df_fin.columns if col != NOTICE_COL]
        for col in financial_cols + [NOTICE_COL]:
            if col not in merged.columns:
                continue
            if merged[col].dtype.kind in ("i", "u", "f"):
                merged.loc[no_match_mask, col] = 0
            else:
                merged.loc[no_match_mask, col] = ""

    merged = merged.rename(columns={NOTICE_COL: "åŒ¹é…è´¢æŠ¥å…¬å‘Šæ—¥æœŸ"})
    merged["æ—¥æœŸ"] = pd.to_datetime(merged["æ—¥æœŸ"], errors="coerce")
    merged = merged.sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)
    merged["æ—¥æœŸ"] = merged["æ—¥æœŸ"].dt.strftime("%Y-%m-%d").fillna("")
    if "åŒ¹é…è´¢æŠ¥å…¬å‘Šæ—¥æœŸ" in merged.columns:
        matched_dates = pd.to_datetime(merged["åŒ¹é…è´¢æŠ¥å…¬å‘Šæ—¥æœŸ"], errors="coerce")
        merged["åŒ¹é…è´¢æŠ¥å…¬å‘Šæ—¥æœŸ"] = matched_dates.dt.strftime("%Y-%m-%d").fillna("")
    return merged


class ProgressManager:
    def __init__(self, output_dir, timestamp, resume_file=None):
        self.output_dir = output_dir
        self.timestamp = timestamp
        self.progress_file = os.path.join(output_dir, f"progress_{timestamp}.json")
        self.processed_codes = set()
        self.total_processed = 0
        self.total_success = 0
        self.total_failed = 0
        if resume_file:
            self._load(resume_file)

    def _load(self, path):
        if not os.path.isabs(path):
            path = os.path.join(self.output_dir, os.path.basename(path))
        if not os.path.exists(path):
            print(f"âš ï¸ è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}")
            return
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        self.timestamp = data.get("timestamp", self.timestamp)
        self.processed_codes = set(data.get("processed_codes", []))
        self.total_processed = data.get("total_processed", 0)
        self.total_success = data.get("total_success", 0)
        self.total_failed = data.get("total_failed", 0)
        self.progress_file = path
        print(f"âœ“ å·²åŠ è½½è¿›åº¦ï¼šå¤„ç†{self.total_processed}å®¶ï¼ŒæˆåŠŸ{self.total_success}ï¼Œå¤±è´¥{self.total_failed}")

    def save(self):
        os.makedirs(self.output_dir, exist_ok=True)
        save_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        data = {
            "timestamp": self.timestamp,
            "save_time": save_time,
            "processed_codes": list(self.processed_codes),
            "total_processed": self.total_processed,
            "total_success": self.total_success,
            "total_failed": self.total_failed,
        }
        with open(self.progress_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


class CompanyProcessor:
    def __init__(self):
        self.mapper = financial_module.FinancialDataMapper(MAPPING_FILE_PATH)
        self.progress = None
        self.output_dir = None
        self.timestamp = None
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.candle_fetcher = CandleFetcher()
        self.index_records = []
        self.session_success_count = 0  # v0.3 æ–°å¢ï¼šæœ¬æ¬¡è¿è¡ŒæˆåŠŸå¤„ç†çš„å…¬å¸æ•°é‡

    def setup(self):
        if RESUME_PROGRESS_FILE:
            self.output_dir = os.path.dirname(RESUME_PROGRESS_FILE)
        else:
            self.output_dir, self.timestamp = ensure_output_dir(OUTPUT_BASE_DIR)
            self.session_timestamp = self.timestamp
        self.progress = ProgressManager(
            output_dir=self.output_dir,
            timestamp=self.timestamp or self.session_timestamp,
            resume_file=RESUME_PROGRESS_FILE
        )
        if RESUME_PROGRESS_FILE:
            self.timestamp = self.progress.timestamp
            self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ç»­ä¼ æ—¶åŠ è½½å·²æœ‰çš„ç´¢å¼•è®°å½•
            index_file = os.path.join(self.output_dir, f"company_index_{self.timestamp}.xlsx")
            if os.path.exists(index_file):
                try:
                    existing_index = pd.read_excel(index_file, sheet_name="å…¬å¸ç´¢å¼•")
                    self.index_records = existing_index.to_dict("records")
                    print(f"âœ“ å·²åŠ è½½ {len(self.index_records)} æ¡å†å²ç´¢å¼•è®°å½•")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
        print(f"\nè¾“å‡ºç›®å½•ï¼š{self.output_dir}")

    def load_stock_dataframe(self):
        print(f"æ­£åœ¨è¯»å–è‚¡ç¥¨åˆ—è¡¨ï¼š{STOCK_LIST_FILE}")
        
        # å®šä¹‰æ¸…ç†å‡½æ•°ï¼Œç›´æ¥åœ¨è¯»å–æ—¶åº”ç”¨
        def clean_name(name):
            if name is None or pd.isna(name):
                return ""
            s = str(name)
            for char in ["\r", "\n", "_x000D_", "_x000d_", "_x000A_", "_x000a_"]:
                s = s.replace(char, "")
            return s.strip()
        
        # è¯»å–æ—¶æŒ‡å®šconvertersï¼Œåœ¨è§£æé˜¶æ®µå°±æ¸…ç†å…¬å¸åç§°
        converters = {}
        if "å…¬å¸åç§°" in pd.read_excel(STOCK_LIST_FILE, sheet_name=SHEET_NAME, nrows=0).columns:
            converters["å…¬å¸åç§°"] = clean_name
        
        df = pd.read_excel(STOCK_LIST_FILE, sheet_name=SHEET_NAME, converters=converters)
        if FILTER_BY_COLUMN and FILTER_BY_COLUMN in df.columns:
            df = df[df[FILTER_BY_COLUMN].notna()]
        if FILTER_CODES:
            if isinstance(FILTER_CODES, (list, tuple, set)):
                codes_iter = FILTER_CODES
            else:
                codes_iter = [FILTER_CODES]
            filter_set = {str(code).strip() for code in codes_iter}
            df = df[df["è‚¡ç¥¨ä»£ç "].astype(str).str.strip().isin(filter_set)]
        if "å¸‚åœºç±»å‹" not in df.columns:
            df["å¸‚åœºç±»å‹"] = df["è‚¡ç¥¨ä»£ç "].apply(financial_module.detect_market_type)
        df = df.reset_index(drop=True)

        if self.progress and self.progress.processed_codes:
            processed = {str(code).strip() for code in self.progress.processed_codes}
            before = len(df)
            df = df[~df["è‚¡ç¥¨ä»£ç "].astype(str).str.strip().isin(processed)].reset_index(drop=True)
            skipped = before - len(df)
            if skipped > 0:
                print(f"â„¹ï¸  å·²è·³è¿‡ {skipped} å®¶å·²å®Œæˆçš„å…¬å¸ï¼ˆåŸºäºè¿›åº¦æ–‡ä»¶ï¼‰")

        return df

    def process_all(self):
        self.setup()
        stock_df = self.load_stock_dataframe()
        if stock_df.empty:
            print("âœ— æ²¡æœ‰å¯å¤„ç†çš„å…¬å¸")
            return

        start_time = time.time()
        for _, row in stock_df.iterrows():
            stock_code = str(row["è‚¡ç¥¨ä»£ç "]).strip()

            company_name = str(row.get("å…¬å¸åç§°", stock_code)).strip() or stock_code
            market = row.get("å¸‚åœºç±»å‹") or financial_module.detect_market_type(stock_code)
            print(f"\n{'='*60}")
            next_seq = self.progress.total_processed + 1
            print(f"[{next_seq}] å¤„ç† {company_name} ({stock_code})")
            print(f"{'='*60}")

            daily_range = row.get("æ—¥KèŒƒå›´")
            weekly_range = row.get("å‘¨KèŒƒå›´")

            success = self.process_single_company(
                sequence=next_seq,
                stock_code=stock_code,
                company_name=company_name,
                market=market,
                daily_range=daily_range,
                weekly_range=weekly_range
            )

            if success:
                self.progress.total_success += 1
                self.session_success_count += 1  # v0.3 æ–°å¢ï¼šç´¯åŠ æœ¬æ¬¡è¿è¡ŒæˆåŠŸæ•°
                
                # v0.3 æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡æš‚åœ
                if PAUSE_AFTER_N_SUCCESS > 0 and self.session_success_count >= PAUSE_AFTER_N_SUCCESS:
                    print(f"\nâ¸ï¸  å·²æˆåŠŸå¤„ç† {self.session_success_count} å®¶å…¬å¸ï¼Œæš‚åœ {PAUSE_MINUTES} åˆ†é’Ÿä»¥é¿å…é¢‘ç¹è¯·æ±‚...")
                    print(f"   æš‚åœæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    pause_end_time = datetime.now() + timedelta(minutes=PAUSE_MINUTES)
                    print(f"   é¢„è®¡æ¢å¤ï¼š{pause_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    time.sleep(PAUSE_MINUTES * 60)
                    self.session_success_count = 0  # é‡ç½®è®¡æ•°å™¨
                    print("âœ“  æš‚åœç»“æŸï¼Œç»§ç»­å¤„ç†...\n")
            else:
                self.progress.total_failed += 1

            self.progress.total_processed += 1
            self.progress.processed_codes.add(stock_code)
            self.progress.save()
            # ç´¢å¼•å·²åœ¨ process_single_company å†…éƒ¨ä¿å­˜ï¼Œæ­¤å¤„ä¸å†é‡å¤è°ƒç”¨

        elapsed = (time.time() - start_time) / 60
        print(f"\nå¤„ç†å®Œæˆï¼è€—æ—¶ {elapsed:.1f} åˆ†é’Ÿ")
        print(f"æˆåŠŸ {self.progress.total_success} å®¶ï¼Œå¤±è´¥ {self.progress.total_failed} å®¶")
        print(f"ç»“æœç›®å½•ï¼š{self.output_dir}")
        
        # v0.3 æ–°å¢ï¼šä¿å­˜å¤±è´¥æ—¥å¿—
        self.save_failure_log()

    def process_single_company(self, sequence, stock_code, company_name, market, daily_range, weekly_range):
        # v0.3 æ–°å¢ï¼šè®°å½•æœ¬æ¬¡å¤„ç†çš„æ—¥K/å‘¨Kå¤±è´¥åŸå› 
        daily_error = ""
        weekly_error = ""
        
        try:
            fin_df, status = self.fetch_financial_data(stock_code, company_name)
            
            # v0.3 æ–°å¢ï¼šè´¢åŠ¡æ•°æ®å¤±è´¥æ—¶ç›´æ¥è·³è¿‡Kçº¿è·å–
            if fin_df is None or fin_df.empty:
                print("  âœ— è´¢åŠ¡æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡Kçº¿æ•°æ®è·å–")
                self.append_index_record(
                    sequence=sequence,
                    company_name=company_name,
                    stock_code=stock_code,
                    market=market,
                    status_info=status,
                    excel_path="",
                    fin_df=pd.DataFrame(),
                    daily_count=0,
                    weekly_count=0,
                    daily_error="æœªè·å–ï¼ˆè´¢åŠ¡æ•°æ®å¤±è´¥ï¼‰",
                    weekly_error="æœªè·å–ï¼ˆè´¢åŠ¡æ•°æ®å¤±è´¥ï¼‰",
                    process_status="å¤±è´¥",
                    failure_reason="è´¢åŠ¡æ•°æ®ä¸ºç©º"
                )
                self.save_company_index()
                return False

            daily_df, daily_count = (None, 0)
            weekly_df, weekly_count = (None, 0)

            if FETCH_DAILY:
                print("  ğŸ“Š è·å–æ—¥Kæ•°æ®...")
                # å¦‚æœExcelä¸­æ²¡æœ‰æŒ‡å®šèŒƒå›´ï¼Œä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
                actual_daily_range = daily_range if (pd.notna(daily_range) and daily_range) else DEFAULT_DAILY_RANGE
                daily_df, daily_count, daily_error = self.fetch_candles(stock_code, market, "daily", actual_daily_range)
                if daily_df is not None and not daily_df.empty:
                    print(f"    âœ“ æ—¥K: {daily_count} æ¡è®°å½•")
                else:
                    print("    âœ— æ—¥Kæ•°æ®ä¸ºç©º")

            if FETCH_WEEKLY:
                if FETCH_DAILY:
                    delay = get_random_delay()
                    print(f"  â±ï¸  ç­‰å¾… {delay:.1f} ç§’åè·å–å‘¨Kæ•°æ®...")
                    time.sleep(delay)
                print("  ğŸ“Š è·å–å‘¨Kæ•°æ®...")
                # å¦‚æœExcelä¸­æ²¡æœ‰æŒ‡å®šèŒƒå›´ï¼Œä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
                actual_weekly_range = weekly_range if (pd.notna(weekly_range) and weekly_range) else DEFAULT_WEEKLY_RANGE
                weekly_df, weekly_count, weekly_error = self.fetch_candles(stock_code, market, "weekly", actual_weekly_range)
                if weekly_df is not None and not weekly_df.empty:
                    print(f"    âœ“ å‘¨K: {weekly_count} æ¡è®°å½•")
                else:
                    print("    âœ— å‘¨Kæ•°æ®ä¸ºç©º")

            aligned_df = align_financial_with_daily(fin_df, daily_df) if FETCH_DAILY else pd.DataFrame()
            if FETCH_DAILY:
                if aligned_df is not None and not aligned_df.empty:
                    latest_date = aligned_df["æ—¥æœŸ"].iloc[0]
                    print(f"  ğŸ”— è´¢æŠ¥Ã—æ—¥Kå¯¹é½å®Œæˆï¼š{len(aligned_df)} è¡Œï¼ˆæœ€æ–°æ—¥æœŸ {latest_date}ï¼‰")
                else:
                    print("  âš  è´¢æŠ¥Ã—æ—¥Kå¯¹é½æ•°æ®ä¸ºç©º")

            excel_path = self.save_company_excel(
                sequence=sequence,
                company_name=company_name,
                stock_code=stock_code,
                fin_df=fin_df,
                daily_df=daily_df,
                weekly_df=weekly_df,
                aligned_df=aligned_df
            )

            self.append_index_record(
                sequence=sequence,
                company_name=company_name,
                stock_code=stock_code,
                market=market,
                status_info=status,
                excel_path=excel_path,
                fin_df=fin_df,
                daily_count=daily_count,
                weekly_count=weekly_count,
                daily_error=daily_error,
                weekly_error=weekly_error,
                process_status="æˆåŠŸ",
                failure_reason=""
            )
            # ç«‹å³ä¿å­˜ç´¢å¼•ï¼Œé¿å…å»¶è¿Ÿå¯¼è‡´ç´¢å¼•æ–‡ä»¶ä¸åŒæ­¥
            self.save_company_index()
            return True

        except IPBlockedException as ip_exc:
            print(f"\nâš ï¸ æ£€æµ‹åˆ°IPå°ç¦ï¼š{ip_exc}")
            self.append_index_record(
                sequence=sequence,
                company_name=company_name,
                stock_code=stock_code,
                market=market,
                status_info=None,
                excel_path="",
                fin_df=pd.DataFrame(),
                daily_count=0,
                weekly_count=0,
                daily_error=daily_error or str(ip_exc),
                weekly_error=weekly_error or str(ip_exc),
                process_status="å¤±è´¥",
                failure_reason="IPå°ç¦"
            )
            self.progress.save()
            self.save_company_index()
            raise
        except Exception as exc:
            print(f"âœ— å¤„ç†å¤±è´¥ï¼š{exc}")
            traceback.print_exc()
            self.append_index_record(
                sequence=sequence,
                company_name=company_name,
                stock_code=stock_code,
                market=market,
                status_info=None,
                excel_path="",
                fin_df=pd.DataFrame(),
                daily_count=0,
                weekly_count=0,
                daily_error=daily_error,
                weekly_error=weekly_error,
                process_status="å¤±è´¥",
                failure_reason=str(exc)
            )
            # å¤±è´¥æ—¶ä¹Ÿç«‹å³ä¿å­˜ç´¢å¼•
            self.save_company_index()
            return False
        finally:
            delay = get_random_delay()
            if delay > 0:
                print(f"  â±ï¸ ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                time.sleep(delay)

    def fetch_financial_data(self, stock_code, company_name):
        result = financial_module.get_single_stock_data(stock_code, company_name)
        df = result.get("data", pd.DataFrame())
        status = result.get("status", {})
        market_type = financial_module.detect_market_type(stock_code)

        if df.empty:
            return df, status

        if "REPORT_DATE" in df.columns:
            df = df.sort_values("REPORT_DATE", ascending=False)

        df = financial_module.format_date_columns(df)
        df = financial_module.add_notice_date_column(df, stock_code, market_type)
        df = financial_module.merge_duplicate_report_dates(df)
        df = self.mapper.map_dataframe(df, market_type, stock_code)
        df = financial_module.add_missing_columns_and_sort(df, market_type, self.mapper)
        df = financial_module.calculate_free_cash_flow(df, stock_code)
        df = financial_module.fill_missing_values_with_zero(df)
        return df, status

    def fetch_candles(self, stock_code, market, period, date_range):
        """v0.3 ä¿®æ”¹ï¼šè¿”å›ä¸‰å…ƒç»„ (df, count, error_message)"""
        df, count, error_msg = self.candle_fetcher.fetch(stock_code, market, period, date_range)
        if df is not None and not df.empty:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")
        return df, count, error_msg

    def save_company_excel(self, sequence, company_name, stock_code, fin_df, daily_df, weekly_df, aligned_df):
        # ç”Ÿæˆå½“å‰ä¿å­˜æ—¶åˆ»çš„æ—¶é—´æˆ³
        current_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{sequence}_{company_name}_{stock_code}_{current_timestamp}.xlsx"
        safe_name = "".join(c if c not in r'\/:*?"<>|' else "_" for c in file_name)
        file_path = os.path.join(self.output_dir, safe_name)

        with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
            fin_df.to_excel(writer, sheet_name="è´¢åŠ¡æ•°æ®", index=False)
            if FETCH_DAILY and daily_df is not None and not daily_df.empty:
                daily_df.to_excel(writer, sheet_name="æ—¥Kæ•°æ®", index=False)
            if FETCH_WEEKLY and weekly_df is not None and not weekly_df.empty:
                weekly_df.to_excel(writer, sheet_name="å‘¨Kæ•°æ®", index=False)
            if aligned_df is not None and not aligned_df.empty:
                aligned_df.to_excel(writer, sheet_name="è´¢æŠ¥_æ—¥Kå¯¹é½", index=False)
        print(f"  âœ“ å·²ä¿å­˜ï¼š{file_path}")
        return file_path

    def append_index_record(
        self, sequence, company_name, stock_code, market, status_info, excel_path,
        fin_df, daily_count, weekly_count, daily_error, weekly_error, process_status, failure_reason
    ):
        """v0.3 ä¿®æ”¹ï¼šå¢åŠ  daily_error å’Œ weekly_error å‚æ•°"""
        data_sources = ""
        indicator_status = "æœªè·å–"
        indicator_error = ""
        indicator_period = ""
        statements_status = "æœªè·å–"
        statements_error = ""
        statements_period = ""

        if status_info:
            data_sources = ", ".join(status_info.get("data_sources", []))
            indicator = status_info.get("indicator", {})
            statements = status_info.get("statements", {})
            indicator_status = "æˆåŠŸ" if indicator.get("success") else "å¤±è´¥"
            indicator_error = indicator.get("error") or ""
            indicator_period = indicator.get("period_used", "")
            statements_status = "æˆåŠŸ" if statements.get("success") else "å¤±è´¥"
            statements_error = statements.get("error") or ""
            statements_period = statements.get("period_used", "")

        record = {
            "åºå·": sequence,
            "å…¬å¸åç§°": company_name,
            "è‚¡ç¥¨ä»£ç ": stock_code,
            "å¸‚åœºç±»å‹": market,
            "çŠ¶æ€": "æˆåŠŸ" if process_status == "æˆåŠŸ" else "å¤±è´¥",
            "Excelæ–‡ä»¶": os.path.basename(excel_path) if excel_path else "",
            "æ•°æ®è¡Œæ•°": len(fin_df) if fin_df is not None else 0,
            "æ•°æ®åˆ—æ•°": len(fin_df.columns) if fin_df is not None and not fin_df.empty else 0,
            "é”™è¯¯ä¿¡æ¯": failure_reason if failure_reason else "",
            "è´¢åŠ¡æŒ‡æ ‡çŠ¶æ€": indicator_status,
            "è´¢åŠ¡æŒ‡æ ‡é”™è¯¯": indicator_error,
            "è´¢åŠ¡æŠ¥è¡¨çŠ¶æ€": statements_status,
            "è´¢åŠ¡æŠ¥è¡¨é”™è¯¯": statements_error,
            "æ•°æ®æ¥æº": data_sources,
            "è´¢åŠ¡æŒ‡æ ‡å‘¨æœŸ": indicator_period,
            "è´¢åŠ¡æŠ¥è¡¨å‘¨æœŸ": statements_period,
            "æ—¥Kæ•°æ®é‡": daily_count,
            "å‘¨Kæ•°æ®é‡": weekly_count,
            "æ—¥Kå¤±è´¥åŸå› ": daily_error,  # v0.3 æ–°å¢
            "å‘¨Kå¤±è´¥åŸå› ": weekly_error,  # v0.3 æ–°å¢
            "å¤„ç†çŠ¶æ€": process_status,
            "å¤±è´¥åŸå› ": failure_reason
        }
        self.index_records.append(record)

    def save_company_index(self):
        if not self.index_records:
            return
        index_df = pd.DataFrame(self.index_records)
        index_file = os.path.join(self.output_dir, f"company_index_{self.timestamp}.xlsx")
        with pd.ExcelWriter(index_file, engine="openpyxl") as writer:
            index_df.to_excel(writer, sheet_name="å…¬å¸ç´¢å¼•", index=False)
        print(f"  âœ“ ç´¢å¼•å·²æ›´æ–°ï¼š{index_file}")

    def save_failure_log(self):
        """v0.3 æ–°å¢ï¼šä¿å­˜Kçº¿è·å–å¤±è´¥çš„è¯¦ç»†æ—¥å¿—"""
        failure_df = self.candle_fetcher.get_failure_log_df()
        if failure_df.empty:
            print("\nâœ“ æ— Kçº¿è·å–å¤±è´¥è®°å½•")
            return
        
        log_file = os.path.join(self.output_dir, f"kline_failure_log_{self.timestamp}.xlsx")
        try:
            with pd.ExcelWriter(log_file, engine="openpyxl") as writer:
                failure_df.to_excel(writer, sheet_name="Kçº¿å¤±è´¥æ—¥å¿—", index=False)
            print(f"\nâœ“ Kçº¿å¤±è´¥æ—¥å¿—å·²ä¿å­˜ï¼š{log_file}")
            print(f"  å…±è®°å½• {len(failure_df)} æ¬¡å¤±è´¥")
        except Exception as e:
            print(f"\nâš ï¸ ä¿å­˜Kçº¿å¤±è´¥æ—¥å¿—æ—¶å‡ºé”™ï¼š{e}")


def main():
    processor = CompanyProcessor()
    try:
        processor.process_all()
    except IPBlockedException:
        print("ç¨‹åºå› IPé™åˆ¶è€Œåœæ­¢ï¼Œè¯·ç¨åé‡è¯•æˆ–æ›´æ¢ç½‘ç»œã€‚")
    except KeyboardInterrupt:
        print("\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚")
    except Exception as exc:
        print(f"\nå‘ç”Ÿæœªé¢„æœŸå¼‚å¸¸ï¼š{exc}")
        traceback.print_exc()
        if processor.progress:
            processor.progress.save()
            processor.save_company_index()


if __name__ == "__main__":
    main()





